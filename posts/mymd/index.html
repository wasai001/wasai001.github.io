<!DOCTYPE html>
<html class="no-js" lang="en-us">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<title>SparkRDD及SparkML在语义分析中的应用 - 大数据时代 | Era of Big Data</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="The usage of SparkRDD and SparkML in the sentimental analysis">
	<meta property="og:title" content="SparkRDD及SparkML在语义分析中的应用" />
<meta property="og:description" content="The usage of SparkRDD and SparkML in the sentimental analysis" />
<meta property="og:type" content="article" />
<meta property="og:url" content="/posts/mymd/" />
<meta property="article:published_time" content="2019-07-08T15:48:56-07:00"/>
<meta property="article:modified_time" content="2019-07-08T15:48:56-07:00"/>

	
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">
	<link rel="stylesheet" href="/css/style.css">
	
	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container">
		<div class="logo">
			<a class="logo__link" href="/" title="大数据时代 | Era of Big Data" rel="home">
				<div class="logo__title">大数据时代 | Era of Big Data</div>
				<div class="logo__tagline">用简单的语言讲大数据 | Talk about big data with plain language</div>
			</a>
		</div>
		
<nav class="menu">
	<button class="menu__btn" aria-haspopup="true" aria-expanded="false" tabindex="0">
		<span class="menu__btn-title" tabindex="-1">Menu</span>
	</button>
	<ul class="menu__list">
		<li class="menu__item">
			<a class="menu__link" href="/posts/englishposts/">English Posts</a>
		</li>
		<li class="menu__item">
			<a class="menu__link" href="/posts/migrate-from-jekyll/">Jekyll migration</a>
		</li>
		<li class="menu__item">
			<a class="menu__link" href="/posts/chineseposts/">中文文章</a>
		</li>
		<li class="menu__item">
			<a class="menu__link" href="/posts/goisforlovers/">(Hu)go Template Primer</a>
		</li>
		<li class="menu__item">
			<a class="menu__link" href="/posts/hugoisforlovers/">Getting Started with Hugo</a>
		</li>
		<li class="menu__item">
			<a class="menu__link" href="/about/">关于本站 | about site</a>
		</li>
	</ul>
</nav>

	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">SparkRDD及SparkML在语义分析中的应用</h1>
			<div class="post__meta meta">
<div class="meta__item-datetime meta__item">
	<svg class="meta__icon icon icon-time" width="16" height="14" viewBox="0 0 30 28"><path d="M15 0C7 0 1 6 1 14s6 14 14 14 14-6 14-14S23 0 15 0zm0 25C9 25 4 20 4 14S9 3 15 3s11 5 11 11-5 11-11 11zm1-18h-2v8.4l6.8 4.4L22 18l-6-3.8V7z"/></svg>
	<time class="meta__text" datetime="2019-07-08T15:48:56">July 08, 2019</time>
</div>

<div class="meta__item-categories meta__item">
	<svg class="meta__icon icon icon-category" width="16" height="16" viewBox="0 0 16 16"><path d="m7 2l1 2h8v11h-16v-13z"/></svg>
	<span class="meta__text"><a class="meta__link" href="/categories/%e4%b8%ad%e6%96%87%e6%96%87%e7%ab%a0" rel="category">中文文章</a></span>
</div>
</div>
		</header>
<div class="post__toc toc">
	<div class="toc__title">Page content</div>
	<div class="toc__menu">
		<nav id="TableOfContents">
<ul>
<li><a href="#unzip-the-file">Unzip the file</a></li>
<li><a href="#load-and-explore-the-data">Load and explore the data</a></li>
<li><a href="#data-preparation">Data Preparation</a></li>
<li><a href="#data-dictionary">Data Dictionary</a></li>
<li><a href="#data-preparation-1-html-decoding">Data Preparation 1: HTML decoding</a></li>
<li><a href="#data-preparation-2-mention">Data Preparation 2: ‘@’mention</a></li>
<li><a href="#data-preparation-3-url-links">Data Preparation 3: URL links</a></li>
<li><a href="#data-preparation-4-utf-8-bom-byte-order-mark">Data Preparation 4: UTF-8 BOM (Byte Order Mark)</a></li>
<li><a href="#data-preparation-5-hashtag-numbers">Data Preparation 5: hashtag / numbers</a></li>
<li><a href="#defining-data-cleaning-function">Defining data cleaning function</a></li>
<li><a href="#saving-cleaned-data-as-csv">Saving cleaned data as csv</a></li>
<li><a href="#sentiment-analysis-with-pyspark">Sentiment Analysis with PySpark</a></li>
<li><a href="#hashingtf-idf-logistic-regression">HashingTF + IDF + Logistic Regression</a></li>
<li><a href="#countvectorizer-idf-logistic-regression">CountVectorizer + IDF + Logistic Regression</a></li>
<li><a href="#n-gram-implementation">N-gram Implementation</a></li>
</ul>
</nav>
	</div>
</div>
<div class="content post__content clearfix">
			

<p>以下为使用SparkRDD储存一百六十万推特用户留言。使用一百六十万数据中的98%数据训练SparkML机器学习模型库中的HashingTF + IDF + Logistic Regression模型。最终使用训练模型预测2%实验数据，并对比准确率。</p>

<h1 id="unzip-the-file">Unzip the file</h1>

<pre><code class="language-python">import zipfile

with zipfile.ZipFile(&quot;./trainingandtestdata/training.1600000.processed.noemoticon.zip&quot;, &quot;r&quot;) as z:
    z.extractall(&quot;./trainingandtestdata&quot;)
</code></pre>

<h1 id="load-and-explore-the-data">Load and explore the data</h1>

<pre><code class="language-python">import pandas as pd  
import numpy as np
import matplotlib.pyplot as plt
plt.style.use('fivethirtyeight')
%matplotlib inline
%config InlineBackend.figure_format = 'retina'

cols = ['sentiment','id','date','query_string','user','text']
df = pd.read_csv(&quot;./trainingandtestdata/training.1600000.processed.noemoticon.csv&quot;,header=None, names=cols, encoding = 'ISO-8859-1')
# df = pd.read_csv(&quot;./trainingandtestdata/training.1600000.processed.noemoticon.csv&quot;,header=None, names=cols,encoding='latin-1')
# above line will be different depending on where you saved your data, and your file name
df.head()
</code></pre>

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }
    
    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>sentiment</th>
      <th>id</th>
      <th>date</th>
      <th>query_string</th>
      <th>user</th>
      <th>text</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0</td>
      <td>1467810369</td>
      <td>Mon Apr 06 22:19:45 PDT 2009</td>
      <td>NO_QUERY</td>
      <td>_TheSpecialOne_</td>
      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0</td>
      <td>1467810672</td>
      <td>Mon Apr 06 22:19:49 PDT 2009</td>
      <td>NO_QUERY</td>
      <td>scotthamilton</td>
      <td>is upset that he can't update his Facebook by ...</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0</td>
      <td>1467810917</td>
      <td>Mon Apr 06 22:19:53 PDT 2009</td>
      <td>NO_QUERY</td>
      <td>mattycus</td>
      <td>@Kenichan I dived many times for the ball. Man...</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0</td>
      <td>1467811184</td>
      <td>Mon Apr 06 22:19:57 PDT 2009</td>
      <td>NO_QUERY</td>
      <td>ElleCTF</td>
      <td>my whole body feels itchy and like its on fire</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0</td>
      <td>1467811193</td>
      <td>Mon Apr 06 22:19:57 PDT 2009</td>
      <td>NO_QUERY</td>
      <td>Karoli</td>
      <td>@nationwideclass no, it's not behaving at all....</td>
    </tr>
  </tbody>
</table>
</div>

<pre><code class="language-python">df.info()
</code></pre>

<pre><code>&lt;class 'pandas.core.frame.DataFrame'&gt;
RangeIndex: 1600000 entries, 0 to 1599999
Data columns (total 6 columns):
sentiment       1600000 non-null int64
id              1600000 non-null int64
date            1600000 non-null object
query_string    1600000 non-null object
user            1600000 non-null object
text            1600000 non-null object
dtypes: int64(2), object(4)
memory usage: 73.2+ MB
</code></pre>

<pre><code class="language-python">df.sentiment.value_counts()
</code></pre>

<pre><code>4    800000
0    800000
Name: sentiment, dtype: int64
</code></pre>

<pre><code class="language-python">df.query_string.value_counts()
</code></pre>

<pre><code>NO_QUERY    1600000
Name: query_string, dtype: int64
</code></pre>

<pre><code class="language-python">df.drop(['id','date','query_string','user'],axis=1,inplace=True)
</code></pre>

<pre><code class="language-python">df.head()
</code></pre>

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }
    
    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>sentiment</th>
      <th>text</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0</td>
      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0</td>
      <td>is upset that he can't update his Facebook by ...</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0</td>
      <td>@Kenichan I dived many times for the ball. Man...</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0</td>
      <td>my whole body feels itchy and like its on fire</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0</td>
      <td>@nationwideclass no, it's not behaving at all....</td>
    </tr>
  </tbody>
</table>
</div>

<pre><code class="language-python">df[df.sentiment == 0].head(10)
</code></pre>

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }
    
    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>sentiment</th>
      <th>text</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0</td>
      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0</td>
      <td>is upset that he can't update his Facebook by ...</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0</td>
      <td>@Kenichan I dived many times for the ball. Man...</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0</td>
      <td>my whole body feels itchy and like its on fire</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0</td>
      <td>@nationwideclass no, it's not behaving at all....</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0</td>
      <td>@Kwesidei not the whole crew</td>
    </tr>
    <tr>
      <th>6</th>
      <td>0</td>
      <td>Need a hug</td>
    </tr>
    <tr>
      <th>7</th>
      <td>0</td>
      <td>@LOLTrish hey  long time no see! Yes.. Rains a...</td>
    </tr>
    <tr>
      <th>8</th>
      <td>0</td>
      <td>@Tatiana_K nope they didn't have it</td>
    </tr>
    <tr>
      <th>9</th>
      <td>0</td>
      <td>@twittera que me muera ?</td>
    </tr>
  </tbody>
</table>
</div>

<pre><code class="language-python">df[df.sentiment == 4].head(10)
</code></pre>

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }
    
    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>sentiment</th>
      <th>text</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>800000</th>
      <td>4</td>
      <td>I LOVE @Health4UandPets u guys r the best!!</td>
    </tr>
    <tr>
      <th>800001</th>
      <td>4</td>
      <td>im meeting up with one of my besties tonight! ...</td>
    </tr>
    <tr>
      <th>800002</th>
      <td>4</td>
      <td>@DaRealSunisaKim Thanks for the Twitter add, S...</td>
    </tr>
    <tr>
      <th>800003</th>
      <td>4</td>
      <td>Being sick can be really cheap when it hurts t...</td>
    </tr>
    <tr>
      <th>800004</th>
      <td>4</td>
      <td>@LovesBrooklyn2 he has that effect on everyone</td>
    </tr>
    <tr>
      <th>800005</th>
      <td>4</td>
      <td>@ProductOfFear You can tell him that I just bu...</td>
    </tr>
    <tr>
      <th>800006</th>
      <td>4</td>
      <td>@r_keith_hill Thans for your response. Ihad al...</td>
    </tr>
    <tr>
      <th>800007</th>
      <td>4</td>
      <td>@KeepinUpWKris I am so jealous, hope you had a...</td>
    </tr>
    <tr>
      <th>800008</th>
      <td>4</td>
      <td>@tommcfly ah, congrats mr fletcher for finally...</td>
    </tr>
    <tr>
      <th>800009</th>
      <td>4</td>
      <td>@e4VoIP I RESPONDED  Stupid cat is helping me ...</td>
    </tr>
  </tbody>
</table>
</div>

<pre><code class="language-python">df[df.sentiment == 0].index
</code></pre>

<pre><code>Int64Index([     0,      1,      2,      3,      4,      5,      6,      7,
                 8,      9,
            ...
            799990, 799991, 799992, 799993, 799994, 799995, 799996, 799997,
            799998, 799999],
           dtype='int64', length=800000)
</code></pre>

<pre><code class="language-python">df[df.sentiment == 4].index
</code></pre>

<pre><code>Int64Index([ 800000,  800001,  800002,  800003,  800004,  800005,  800006,
             800007,  800008,  800009,
            ...
            1599990, 1599991, 1599992, 1599993, 1599994, 1599995, 1599996,
            1599997, 1599998, 1599999],
           dtype='int64', length=800000)
</code></pre>

<pre><code class="language-python">df['sentiment'] = df['sentiment'].map({0: 0, 4: 1})
</code></pre>

<pre><code class="language-python">df.sentiment.value_counts()
</code></pre>

<pre><code>1    800000
0    800000
Name: sentiment, dtype: int64
</code></pre>

<h1 id="data-preparation">Data Preparation</h1>

<pre><code class="language-python">df['pre_clean_len'] = [len(t) for t in df.text]
</code></pre>

<pre><code class="language-python">df.head()
</code></pre>

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }
    
    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>sentiment</th>
      <th>text</th>
      <th>pre_clean_len</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0</td>
      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>
      <td>115</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0</td>
      <td>is upset that he can't update his Facebook by ...</td>
      <td>111</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0</td>
      <td>@Kenichan I dived many times for the ball. Man...</td>
      <td>89</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0</td>
      <td>my whole body feels itchy and like its on fire</td>
      <td>47</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0</td>
      <td>@nationwideclass no, it's not behaving at all....</td>
      <td>111</td>
    </tr>
  </tbody>
</table>
</div>

<h1 id="data-dictionary">Data Dictionary</h1>

<pre><code class="language-python">from pprint import pprint
data_dict = {
    'sentiment':{
        'type':df.sentiment.dtype,
        'description':'sentiment class - 0:negative, 1:positive'
    },
    'text':{
        'type':df.text.dtype,
        'description':'tweet text'
    },
    'pre_clean_len':{
        'type':df.pre_clean_len.dtype,
        'description':'Length of the tweet before cleaning'
    },
    'dataset_shape':df.shape
}

pprint(data_dict)
</code></pre>

<pre><code>{'dataset_shape': (1600000, 3),
 'pre_clean_len': {'description': 'Length of the tweet before cleaning',
                   'type': dtype('int64')},
 'sentiment': {'description': 'sentiment class - 0:negative, 1:positive',
               'type': dtype('int64')},
 'text': {'description': 'tweet text', 'type': dtype('O')}}
</code></pre>

<pre><code class="language-python">fig, ax = plt.subplots(figsize=(5, 5))
plt.boxplot(df.pre_clean_len)
plt.show()
</code></pre>

<p><img src="output_20_0.png" alt="png" /></p>

<pre><code class="language-python">df[df.pre_clean_len &gt; 140].head(10)
</code></pre>

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }
    
    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>sentiment</th>
      <th>text</th>
      <th>pre_clean_len</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>213</th>
      <td>0</td>
      <td>Awwh babs... you look so sad underneith that s...</td>
      <td>142</td>
    </tr>
    <tr>
      <th>279</th>
      <td>0</td>
      <td>Whinging. My client&amp;amp;boss don't understand ...</td>
      <td>145</td>
    </tr>
    <tr>
      <th>343</th>
      <td>0</td>
      <td>@TheLeagueSF Not Fun &amp;amp; Furious? The new ma...</td>
      <td>145</td>
    </tr>
    <tr>
      <th>400</th>
      <td>0</td>
      <td>#3 woke up and was having an accident - &amp;quot;...</td>
      <td>144</td>
    </tr>
    <tr>
      <th>464</th>
      <td>0</td>
      <td>My bathtub drain is fired: it haz 1 job 2 do, ...</td>
      <td>146</td>
    </tr>
    <tr>
      <th>492</th>
      <td>0</td>
      <td>pears &amp;amp; Brie, bottle of Cabernet, and &amp;quo...</td>
      <td>150</td>
    </tr>
    <tr>
      <th>747</th>
      <td>0</td>
      <td>Have an invite for &amp;quot;Healthy Dining&amp;quot; ...</td>
      <td>141</td>
    </tr>
    <tr>
      <th>957</th>
      <td>0</td>
      <td>Damnit I was really digging this season of Rea...</td>
      <td>141</td>
    </tr>
    <tr>
      <th>1064</th>
      <td>0</td>
      <td>Why do I keep looking...I know that what I rea...</td>
      <td>141</td>
    </tr>
    <tr>
      <th>1071</th>
      <td>0</td>
      <td>Used the term &amp;quot;Fail Whale&amp;quot; to a clie...</td>
      <td>148</td>
    </tr>
  </tbody>
</table>
</div>

<h1 id="data-preparation-1-html-decoding">Data Preparation 1: HTML decoding</h1>

<pre><code class="language-python">df.text[279]
</code></pre>

<pre><code>&quot;Whinging. My client&amp;amp;boss don't understand English well. Rewrote some text unreadable. It's written by v. good writer&amp;amp;reviewed correctly. &quot;
</code></pre>

<pre><code class="language-python">from bs4 import BeautifulSoup
example1 = BeautifulSoup(df.text[279], 'lxml')
print(example1.get_text())
</code></pre>

<pre><code>Whinging. My client&amp;boss don't understand English well. Rewrote some text unreadable. It's written by v. good writer&amp;reviewed correctly. 
</code></pre>

<h1 id="data-preparation-2-mention">Data Preparation 2: ‘@’mention</h1>

<pre><code class="language-python">df.text[343]
</code></pre>

<pre><code>'@TheLeagueSF Not Fun &amp;amp; Furious? The new mantra for the Bay 2 Breakers? It was getting 2 rambunctious;the city overreacted &amp;amp; clamped down '
</code></pre>

<pre><code class="language-python">import re
re.sub(r'@[A-Za-z0-9]+','',df.text[343])
</code></pre>

<pre><code>' Not Fun &amp;amp; Furious? The new mantra for the Bay 2 Breakers? It was getting 2 rambunctious;the city overreacted &amp;amp; clamped down '
</code></pre>

<h1 id="data-preparation-3-url-links">Data Preparation 3: URL links</h1>

<pre><code class="language-python">df.text[0]
</code></pre>

<pre><code>&quot;@switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D&quot;
</code></pre>

<pre><code class="language-python">re.sub('https?://[A-Za-z0-9./]+','',df.text[0])
</code></pre>

<pre><code>&quot;@switchfoot  - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D&quot;
</code></pre>

<h1 id="data-preparation-4-utf-8-bom-byte-order-mark">Data Preparation 4: UTF-8 BOM (Byte Order Mark)</h1>

<pre><code class="language-python">df.text[226]
</code></pre>

<pre><code>'Tuesday?ll start with reflection ?n then a lecture in Stress reducing techniques. That sure might become very useful for us accompaniers '
</code></pre>

<p>Skip this step</p>

<h1 id="data-preparation-5-hashtag-numbers">Data Preparation 5: hashtag / numbers</h1>

<pre><code class="language-python">df.text[175]
</code></pre>

<pre><code>&quot;@machineplay I'm so sorry you're having to go through this. Again.  #therapyfail&quot;
</code></pre>

<pre><code class="language-python">re.sub(&quot;[^a-zA-Z]&quot;, &quot; &quot;, df.text[175])
</code></pre>

<pre><code>' machineplay I m so sorry you re having to go through this  Again    therapyfail'
</code></pre>

<h1 id="defining-data-cleaning-function">Defining data cleaning function</h1>

<pre><code class="language-python">from nltk.tokenize import WordPunctTokenizer
tok = WordPunctTokenizer()
pat1 = r'@[A-Za-z0-9]+'
pat2 = r'https?://[A-Za-z0-9./]+'
combined_pat = r'|'.join((pat1, pat2))

def tweet_cleaner(text):
    soup = BeautifulSoup(text, 'lxml')
    souped = soup.get_text()
    stripped = re.sub(combined_pat, '', souped)
    try:
        clean = stripped.decode(&quot;utf-8-sig&quot;).replace(u&quot;\ufffd&quot;, &quot;?&quot;)
    except:
        clean = stripped
    letters_only = re.sub(&quot;[^a-zA-Z]&quot;, &quot; &quot;, clean)
    lower_case = letters_only.lower()
    # During the letters_only process two lines above, it has created unnecessay white spaces,
    # I will tokenize and join together to remove unneccessary white spaces
    words = tok.tokenize(lower_case)
    return (&quot; &quot;.join(words)).strip()

testing = df.text[:100]

test_result = []
for t in testing:
    test_result.append(tweet_cleaner(t))
test_result
</code></pre>

<pre><code>['awww that s a bummer you shoulda got david carr of third day to do it d',
 'is upset that he can t update his facebook by texting it and might cry as a result school today also blah',
 'i dived many times for the ball managed to save the rest go out of bounds',
 'my whole body feels itchy and like its on fire',
 'no it s not behaving at all i m mad why am i here because i can t see you all over there',
 'not the whole crew',
 'need a hug',
 'hey long time no see yes rains a bit only a bit lol i m fine thanks how s you',
 'k nope they didn t have it',
 'que me muera',
 'spring break in plain city it s snowing',
 'i just re pierced my ears',
 'i couldn t bear to watch it and i thought the ua loss was embarrassing',
 'it it counts idk why i did either you never talk to me anymore',
 'i would ve been the first but i didn t have a gun not really though zac snyder s just a doucheclown',
 'i wish i got to watch it with you i miss you and how was the premiere',
 'hollis death scene will hurt me severely to watch on film wry is directors cut not out now',
 'about to file taxes',
 'ahh ive always wanted to see rent love the soundtrack',
 'oh dear were you drinking out of the forgotten table drinks',
 'i was out most of the day so didn t get much done',
 'one of my friend called me and asked to meet with her at mid valley today but i ve no time sigh',
 'barista i baked you a cake but i ated it',
 'this week is not going as i had hoped',
 'blagh class at tomorrow',
 'i hate when i have to call and wake people up',
 'just going to cry myself to sleep after watching marley and me',
 'im sad now miss lilly',
 'ooooh lol that leslie and ok i won t do it again so leslie won t get mad again',
 'meh almost lover is the exception this track gets me depressed every time',
 'some hacked my account on aim now i have to make a new one',
 'i want to go to promote gear and groove but unfornately no ride there i may b going to the one in anaheim in may though',
 'thought sleeping in was an option tomorrow but realizing that it now is not evaluations in the morning and work in the afternoon',
 'awe i love you too am here i miss you',
 'i cry my asian eyes to sleep at night',
 'ok i m sick and spent an hour sitting in the shower cause i was too sick to stand and held back the puke like a champ bed now',
 'ill tell ya the story later not a good day and ill be workin for like three more hours',
 'sorry bed time came here gmt',
 'i don t either its depressing i don t think i even want to know about the kids in suitcases',
 'bed class work gym or then class another day that s gonna fly by i miss my girlfriend',
 'really don t feel like getting up today but got to study to for tomorrows practical exam',
 'he s the reason for the teardrops on my guitar the only one who has enough of me to break my heart',
 'sad sad sad i don t know why but i hate this feeling i wanna sleep and i still can t',
 'awww i soo wish i was there to see you finally comfortable im sad that i missed it',
 'falling asleep just heard about that tracy girl s body being found how sad my heart breaks for that family',
 'yay i m happy for you with your job but that also means less time for me and you',
 'just checked my user timeline on my blackberry it looks like the twanking is still happening are ppl still having probs w bgs and uids',
 'oh man was ironing s fave top to wear to a meeting burnt it',
 'is strangely sad about lilo and samro breaking up',
 'oh i m so sorry i didn t think about that before retweeting',
 'broadband plan a massive broken promise via www diigo com tautao still waiting for broadband we are',
 'wow tons of replies from you may have to unfollow so i can see my friends tweets you re scrolling the feed a lot',
 'our duck and chicken are taking wayyy too long to hatch',
 'put vacation photos online a few yrs ago pc crashed and now i forget the name of the site',
 'i need a hug',
 'not sure what they are only that they are pos as much as i want to i dont think can trade away company assets sorry andy',
 'i hate when that happens',
 'i have a sad feeling that dallas is not going to show up i gotta say though you d think more shows would use music from the game mmm',
 'ugh degrees tomorrow',
 'where did u move to i thought u were already in sd hmmm random u found me glad to hear yer doing well',
 'i miss my ps it s out of commission wutcha playing have you copped blood on the sand',
 'just leaving the parking lot of work',
 'the life is cool but not for me',
 'sadly though i ve never gotten to experience the post coitus cigarette before and now i never will',
 'i had such a nice day too bad the rain comes in tomorrow at am',
 'too bad i won t be around i lost my job and can t even pay my phone bill lmao aw shucks',
 'damm back to school tomorrow',
 'mo jobs no money how in the hell is min wage here f n clams an hour',
 'not forever see you soon',
 'algonquin agreed i saw the failwhale allllll day today',
 'oh haha dude i dont really look at em unless someone says hey i added you sorry i m so terrible at that i need a pop up',
 'i m sure you re right i need to start working out with you and the nikster or jared at least',
 'i really hate how people diss my bands trace is clearly not ugly',
 'gym attire today was puma singlet adidas shorts and black business socks and leather shoes lucky did not run into any cute girls',
 'why won t you show my location',
 'no picnic my phone smells like citrus',
 'my donkey is sensitive about such comments nevertheless he d and me d be glad to see your mug asap charger is still awol',
 'no new csi tonight fml',
 'i think my arms are sore from tennis',
 'wonders why someone that u like so much can make you so unhappy in a split seccond depressed',
 'sleep soon i just hate saying bye and see you tomorrow for the night',
 'just got ur newsletter those fares really are unbelievable shame i already booked and paid for mine',
 'missin the boo',
 'me too itm',
 'damn i don t have any chalk my chalkboard is useless',
 'had a blast at the getty villa but hates that she s had a sore throat all day it s just getting worse too',
 'hey missed ya at the meeting sup mama',
 'my tummy hurts i wonder if the hypnosis has anything to do with it if so it s working i get it stop smoking',
 'why is it always the fat ones',
 'sorry babe my fam annoys me too thankfully they re asleep right now muahaha evil laugh',
 'i should have paid more attention when we covered photoshop in my webpage design class in undergrad',
 'wednesday my b day don t know what do',
 'poor cameron the hills',
 'pray for me please the ex is threatening to start sh at my our babies st birthday party what a jerk and i still have a headache',
 'hmm do u really enjoy being with him if the problems are too constants u should think things more find someone ulike',
 'strider is a sick little puppy',
 'so rylee grace wana go steve s party or not sadly since its easter i wnt b able do much but ohh well',
 'hey i actually won one of my bracket pools too bad it wasn t the one for money',
 'you don t follow me either and i work for you',
 'a bad nite for the favorite teams astros and spartans lose the nite out with t w was good']
</code></pre>

<pre><code class="language-python">nums = [0,400000,800000,1200000,1600000]
print(&quot;Cleaning and parsing the tweets...\n&quot;)
clean_tweet_texts = []
for i in range(nums[0],nums[1]):
    if( (i+1)%10000 == 0 ):
        print(&quot;Tweets %d of %d has been processed&quot; % ( i+1, nums[1] ))                                                                    
    clean_tweet_texts.append(tweet_cleaner(df['text'][i]))
</code></pre>

<pre><code>Cleaning and parsing the tweets...

Tweets 10000 of 400000 has been processed
Tweets 20000 of 400000 has been processed
Tweets 30000 of 400000 has been processed
Tweets 40000 of 400000 has been processed
Tweets 50000 of 400000 has been processed
Tweets 60000 of 400000 has been processed
Tweets 70000 of 400000 has been processed
Tweets 80000 of 400000 has been processed
Tweets 90000 of 400000 has been processed
Tweets 100000 of 400000 has been processed
Tweets 110000 of 400000 has been processed
Tweets 120000 of 400000 has been processed
Tweets 130000 of 400000 has been processed
Tweets 140000 of 400000 has been processed
Tweets 150000 of 400000 has been processed
Tweets 160000 of 400000 has been processed
Tweets 170000 of 400000 has been processed
Tweets 180000 of 400000 has been processed
Tweets 190000 of 400000 has been processed
Tweets 200000 of 400000 has been processed
Tweets 210000 of 400000 has been processed
Tweets 220000 of 400000 has been processed
Tweets 230000 of 400000 has been processed
Tweets 240000 of 400000 has been processed
Tweets 250000 of 400000 has been processed
Tweets 260000 of 400000 has been processed
Tweets 270000 of 400000 has been processed
Tweets 280000 of 400000 has been processed
Tweets 290000 of 400000 has been processed
Tweets 300000 of 400000 has been processed
Tweets 310000 of 400000 has been processed
Tweets 320000 of 400000 has been processed
Tweets 330000 of 400000 has been processed
Tweets 340000 of 400000 has been processed
Tweets 350000 of 400000 has been processed
Tweets 360000 of 400000 has been processed
Tweets 370000 of 400000 has been processed
Tweets 380000 of 400000 has been processed
Tweets 390000 of 400000 has been processed
Tweets 400000 of 400000 has been processed
</code></pre>

<pre><code class="language-python">len(clean_tweet_texts)
</code></pre>

<pre><code>400000
</code></pre>

<pre><code class="language-python">print(&quot;Cleaning and parsing the tweets...\n&quot;)
for i in range(nums[1],nums[2]):
    if( (i+1)%10000 == 0 ):
        print(&quot;Tweets %d of %d has been processed&quot; % ( i+1, nums[2] ))                                                                   
    clean_tweet_texts.append(tweet_cleaner(df['text'][i]))
</code></pre>

<pre><code>Cleaning and parsing the tweets...

Tweets 410000 of 800000 has been processed
Tweets 420000 of 800000 has been processed
Tweets 430000 of 800000 has been processed
Tweets 440000 of 800000 has been processed
Tweets 450000 of 800000 has been processed
Tweets 460000 of 800000 has been processed
Tweets 470000 of 800000 has been processed
Tweets 480000 of 800000 has been processed
Tweets 490000 of 800000 has been processed
Tweets 500000 of 800000 has been processed
Tweets 510000 of 800000 has been processed
Tweets 520000 of 800000 has been processed
Tweets 530000 of 800000 has been processed
Tweets 540000 of 800000 has been processed
Tweets 550000 of 800000 has been processed
Tweets 560000 of 800000 has been processed
Tweets 570000 of 800000 has been processed
Tweets 580000 of 800000 has been processed
Tweets 590000 of 800000 has been processed
Tweets 600000 of 800000 has been processed
Tweets 610000 of 800000 has been processed
Tweets 620000 of 800000 has been processed
Tweets 630000 of 800000 has been processed
Tweets 640000 of 800000 has been processed
Tweets 650000 of 800000 has been processed
Tweets 660000 of 800000 has been processed
Tweets 670000 of 800000 has been processed
Tweets 680000 of 800000 has been processed
Tweets 690000 of 800000 has been processed
Tweets 700000 of 800000 has been processed
Tweets 710000 of 800000 has been processed
Tweets 720000 of 800000 has been processed
Tweets 730000 of 800000 has been processed
Tweets 740000 of 800000 has been processed
Tweets 750000 of 800000 has been processed
Tweets 760000 of 800000 has been processed
Tweets 770000 of 800000 has been processed
Tweets 780000 of 800000 has been processed
Tweets 790000 of 800000 has been processed
Tweets 800000 of 800000 has been processed
</code></pre>

<pre><code class="language-python">len(clean_tweet_texts)
</code></pre>

<pre><code>800000
</code></pre>

<pre><code class="language-python">print(&quot;Cleaning and parsing the tweets...\n&quot;)
for i in range(nums[2],nums[3]):
    if( (i+1)%10000 == 0 ):
        print(&quot;Tweets %d of %d has been processed&quot; % ( i+1, nums[3] ) )                                                                   
    clean_tweet_texts.append(tweet_cleaner(df['text'][i]))
</code></pre>

<pre><code>Cleaning and parsing the tweets...

Tweets 810000 of 1200000 has been processed
Tweets 820000 of 1200000 has been processed
Tweets 830000 of 1200000 has been processed
Tweets 840000 of 1200000 has been processed
Tweets 850000 of 1200000 has been processed
Tweets 860000 of 1200000 has been processed
Tweets 870000 of 1200000 has been processed
Tweets 880000 of 1200000 has been processed
Tweets 890000 of 1200000 has been processed
Tweets 900000 of 1200000 has been processed
Tweets 910000 of 1200000 has been processed
Tweets 920000 of 1200000 has been processed
Tweets 930000 of 1200000 has been processed
Tweets 940000 of 1200000 has been processed
Tweets 950000 of 1200000 has been processed
Tweets 960000 of 1200000 has been processed
Tweets 970000 of 1200000 has been processed
Tweets 980000 of 1200000 has been processed
Tweets 990000 of 1200000 has been processed
Tweets 1000000 of 1200000 has been processed
Tweets 1010000 of 1200000 has been processed
Tweets 1020000 of 1200000 has been processed
Tweets 1030000 of 1200000 has been processed
Tweets 1040000 of 1200000 has been processed
Tweets 1050000 of 1200000 has been processed
Tweets 1060000 of 1200000 has been processed
Tweets 1070000 of 1200000 has been processed
Tweets 1080000 of 1200000 has been processed
Tweets 1090000 of 1200000 has been processed
Tweets 1100000 of 1200000 has been processed
Tweets 1110000 of 1200000 has been processed
Tweets 1120000 of 1200000 has been processed
Tweets 1130000 of 1200000 has been processed
Tweets 1140000 of 1200000 has been processed
Tweets 1150000 of 1200000 has been processed
Tweets 1160000 of 1200000 has been processed
Tweets 1170000 of 1200000 has been processed
Tweets 1180000 of 1200000 has been processed
Tweets 1190000 of 1200000 has been processed
Tweets 1200000 of 1200000 has been processed
</code></pre>

<pre><code class="language-python">len(clean_tweet_texts)
</code></pre>

<pre><code>1200000
</code></pre>

<pre><code class="language-python">print(&quot;Cleaning and parsing the tweets...\n&quot;)
for i in range(nums[3],nums[4]):
    if( (i+1)%10000 == 0 ):
        print(&quot;Tweets %d of %d has been processed&quot; % ( i+1, nums[4] ) )                                                                   
    clean_tweet_texts.append(tweet_cleaner(df['text'][i]))
</code></pre>

<pre><code>Cleaning and parsing the tweets...

Tweets 1210000 of 1600000 has been processed
Tweets 1220000 of 1600000 has been processed
Tweets 1230000 of 1600000 has been processed
Tweets 1240000 of 1600000 has been processed
Tweets 1250000 of 1600000 has been processed
Tweets 1260000 of 1600000 has been processed
Tweets 1270000 of 1600000 has been processed
Tweets 1280000 of 1600000 has been processed
Tweets 1290000 of 1600000 has been processed
Tweets 1300000 of 1600000 has been processed
Tweets 1310000 of 1600000 has been processed
Tweets 1320000 of 1600000 has been processed
Tweets 1330000 of 1600000 has been processed
Tweets 1340000 of 1600000 has been processed
Tweets 1350000 of 1600000 has been processed
Tweets 1360000 of 1600000 has been processed
Tweets 1370000 of 1600000 has been processed
Tweets 1380000 of 1600000 has been processed
Tweets 1390000 of 1600000 has been processed
Tweets 1400000 of 1600000 has been processed
Tweets 1410000 of 1600000 has been processed
Tweets 1420000 of 1600000 has been processed
Tweets 1430000 of 1600000 has been processed
Tweets 1440000 of 1600000 has been processed
Tweets 1450000 of 1600000 has been processed
Tweets 1460000 of 1600000 has been processed
Tweets 1470000 of 1600000 has been processed
Tweets 1480000 of 1600000 has been processed
Tweets 1490000 of 1600000 has been processed
Tweets 1500000 of 1600000 has been processed
Tweets 1510000 of 1600000 has been processed
Tweets 1520000 of 1600000 has been processed
Tweets 1530000 of 1600000 has been processed
Tweets 1540000 of 1600000 has been processed
Tweets 1550000 of 1600000 has been processed
Tweets 1560000 of 1600000 has been processed
Tweets 1570000 of 1600000 has been processed
Tweets 1580000 of 1600000 has been processed
Tweets 1590000 of 1600000 has been processed
Tweets 1600000 of 1600000 has been processed
</code></pre>

<pre><code class="language-python">len(clean_tweet_texts)
</code></pre>

<pre><code>1600000
</code></pre>

<h1 id="saving-cleaned-data-as-csv">Saving cleaned data as csv</h1>

<pre><code class="language-python">clean_df = pd.DataFrame(clean_tweet_texts,columns=['text'])
clean_df['target'] = df.sentiment
clean_df.head()
</code></pre>

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }
    
    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>text</th>
      <th>target</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>awww that s a bummer you shoulda got david car...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>is upset that he can t update his facebook by ...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>i dived many times for the ball managed to sav...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>my whole body feels itchy and like its on fire</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>no it s not behaving at all i m mad why am i h...</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>

<pre><code class="language-python">clean_df.to_csv('clean_tweet.csv',encoding='ISO-8859-1')
</code></pre>

<h1 id="sentiment-analysis-with-pyspark">Sentiment Analysis with PySpark</h1>

<pre><code class="language-python">import findspark
findspark.init()
import pyspark as ps
import warnings
from pyspark.sql import SQLContext
</code></pre>

<pre><code class="language-python">try:
    # create SparkContext on all CPUs available: in my case I have 4 CPUs on my laptop
    sc = ps.SparkContext('local[*]')
    sqlContext = SQLContext(sc)
    print(&quot;Just created a SparkContext&quot;)
except ValueError:
    warnings.warn(&quot;SparkContext already exists in this scope&quot;)
</code></pre>

<pre><code>Just created a SparkContext
</code></pre>

<pre><code class="language-python">sc.master
</code></pre>

<pre><code>'local[*]'
</code></pre>

<pre><code class="language-python">df = sqlContext.read.format('com.databricks.spark.csv').options(header='true', inferschema='true').load('clean_tweet.csv')
type(df)
</code></pre>

<pre><code>pyspark.sql.dataframe.DataFrame
</code></pre>

<pre><code class="language-python">df.show(5)
</code></pre>

<pre><code>+---+--------------------+------+
|_c0|                text|target|
+---+--------------------+------+
|  0|awww that s a bum...|     0|
|  1|is upset that he ...|     0|
|  2|i dived many time...|     0|
|  3|my whole body fee...|     0|
|  4|no it s not behav...|     0|
+---+--------------------+------+
only showing top 5 rows
</code></pre>

<p>​</p>

<pre><code class="language-python">df = df.dropna()
df.count()
</code></pre>

<pre><code>1596753
</code></pre>

<pre><code class="language-python">(train_set, val_set, test_set) = df.randomSplit([0.98, 0.01, 0.01], seed = 2000)
</code></pre>

<h1 id="hashingtf-idf-logistic-regression">HashingTF + IDF + Logistic Regression</h1>

<pre><code class="language-python">from pyspark.ml.feature import HashingTF, IDF, Tokenizer, CountVectorizer
from pyspark.ml.feature import StringIndexer
from pyspark.ml import Pipeline
from pyspark.ml.classification import LogisticRegression
from pyspark.ml.evaluation import BinaryClassificationEvaluator
</code></pre>

<pre><code class="language-python">from pyspark.ml.feature import HashingTF, IDF, Tokenizer
from pyspark.ml.feature import StringIndexer
from pyspark.ml import Pipeline

tokenizer = Tokenizer(inputCol=&quot;text&quot;, outputCol=&quot;words&quot;)
hashtf = HashingTF(numFeatures=2**16, inputCol=&quot;words&quot;, outputCol='tf')
idf = IDF(inputCol='tf', outputCol=&quot;features&quot;, minDocFreq=5) #minDocFreq: remove sparse terms
label_stringIdx = StringIndexer(inputCol = &quot;target&quot;, outputCol = &quot;label&quot;)
pipeline = Pipeline(stages=[tokenizer, hashtf, idf, label_stringIdx])

pipelineFit = pipeline.fit(train_set)
train_df = pipelineFit.transform(train_set)
val_df = pipelineFit.transform(val_set)
train_df.show(5)
</code></pre>

<pre><code>+---+--------------------+------+--------------------+--------------------+--------------------+-----+
|_c0|                text|target|               words|                  tf|            features|label|
+---+--------------------+------+--------------------+--------------------+--------------------+-----+
|  0|awww that s a bum...|     0|[awww, that, s, a...|(65536,[8436,8847...|(65536,[8436,8847...|  0.0|
|  1|is upset that he ...|     0|[is, upset, that,...|(65536,[1444,2071...|(65536,[1444,2071...|  0.0|
|  2|i dived many time...|     0|[i, dived, many, ...|(65536,[2548,2888...|(65536,[2548,2888...|  0.0|
|  3|my whole body fee...|     0|[my, whole, body,...|(65536,[158,11650...|(65536,[158,11650...|  0.0|
|  4|no it s not behav...|     0|[no, it, s, not, ...|(65536,[1968,4488...|(65536,[1968,4488...|  0.0|
+---+--------------------+------+--------------------+--------------------+--------------------+-----+
only showing top 5 rows
</code></pre>

<p>​</p>

<pre><code class="language-python">from pyspark.ml.classification import LogisticRegression
lr = LogisticRegression(maxIter=100)
lrModel = lr.fit(train_df)
predictions = lrModel.transform(val_df)

from pyspark.ml.evaluation import BinaryClassificationEvaluator
evaluator = BinaryClassificationEvaluator(rawPredictionCol=&quot;rawPrediction&quot;)
evaluator.evaluate(predictions)
</code></pre>

<pre><code>0.8611638209038138
</code></pre>

<pre><code class="language-python">evaluator.getMetricName()
</code></pre>

<pre><code>'areaUnderROC'
</code></pre>

<pre><code class="language-python">accuracy = predictions.filter(predictions.label == predictions.prediction).count() / float(val_set.count())
accuracy
</code></pre>

<pre><code>0.7919382395747643
</code></pre>

<pre><code class="language-python">predictions_t = lrModel.transform(train_df)
accuracy_t = predictions_t.filter(predictions_t.label == predictions_t.prediction).count() / float(train_set.count())
accuracy_t
</code></pre>

<pre><code>0.8094149371229935
</code></pre>

<h1 id="countvectorizer-idf-logistic-regression">CountVectorizer + IDF + Logistic Regression</h1>

<pre><code class="language-python">from pyspark.ml.feature import CountVectorizer

tokenizer = Tokenizer(inputCol=&quot;text&quot;, outputCol=&quot;words&quot;)
cv = CountVectorizer(vocabSize=2**16, inputCol=&quot;words&quot;, outputCol='cv')
idf = IDF(inputCol='cv', outputCol=&quot;features&quot;, minDocFreq=5) #minDocFreq: remove sparse terms
label_stringIdx = StringIndexer(inputCol = &quot;target&quot;, outputCol = &quot;label&quot;)
lr = LogisticRegression(maxIter=100)
pipeline = Pipeline(stages=[tokenizer, cv, idf, label_stringIdx, lr])

pipelineFit = pipeline.fit(train_set)
predictions = pipelineFit.transform(val_set)
accuracy = predictions.filter(predictions.label == predictions.prediction).count() / float(val_set.count())
roc_auc = evaluator.evaluate(predictions)

print(&quot;Accuracy Score: {0:.4f}&quot;.format(accuracy))
print(&quot;ROC-AUC: {0:.4f}&quot;.format(roc_auc))
</code></pre>

<pre><code>Accuracy Score: 0.7951
ROC-AUC: 0.8668
</code></pre>

<pre><code class="language-python">predictions_t2 = pipelineFit.transform(train_set)
accuracy_t2 = predictions_t2.filter(predictions_t2.label == predictions_t2.prediction).count() / float(train_set.count())
accuracy_t2
</code></pre>

<pre><code>0.815451513137716
</code></pre>

<h1 id="n-gram-implementation">N-gram Implementation</h1>

<pre><code class="language-python">from pyspark.ml.feature import NGram, VectorAssembler
from pyspark.ml.feature import ChiSqSelector

def build_trigrams(inputCol=[&quot;text&quot;,&quot;target&quot;], n=3):
    tokenizer = [Tokenizer(inputCol=&quot;text&quot;, outputCol=&quot;words&quot;)]
    ngrams = [
        NGram(n=i, inputCol=&quot;words&quot;, outputCol=&quot;{0}_grams&quot;.format(i))
        for i in range(1, n + 1)
    ]

    cv = [
        CountVectorizer(vocabSize=2**14,inputCol=&quot;{0}_grams&quot;.format(i),
            outputCol=&quot;{0}_tf&quot;.format(i))
        for i in range(1, n + 1)
    ]
    idf = [IDF(inputCol=&quot;{0}_tf&quot;.format(i), outputCol=&quot;{0}_tfidf&quot;.format(i), minDocFreq=5) for i in range(1, n + 1)]

    assembler = [VectorAssembler(
        inputCols=[&quot;{0}_tfidf&quot;.format(i) for i in range(1, n + 1)],
        outputCol=&quot;rawFeatures&quot;
    )]
    label_stringIdx = [StringIndexer(inputCol = &quot;target&quot;, outputCol = &quot;label&quot;)]
    selector = [ChiSqSelector(numTopFeatures=2**14,featuresCol='rawFeatures', outputCol=&quot;features&quot;)]
    lr = [LogisticRegression(maxIter=100)]
    return Pipeline(stages=tokenizer + ngrams + cv + idf+ assembler + label_stringIdx+selector+lr)
</code></pre>

<pre><code class="language-python">trigram_pipelineFit = build_trigrams().fit(train_set)
predictions = trigram_pipelineFit.transform(val_set)
accuracy = predictions.filter(predictions.label == predictions.prediction).count() / float(val_set.count())
roc_auc = evaluator.evaluate(predictions)

# print accuracy, roc_auc
print(&quot;Accuracy Score: {0:.4f}&quot;.format(accuracy))
print(&quot;ROC-AUC: {0:.4f}&quot;.format(roc_auc))
</code></pre>

<pre><code>Accuracy Score: 0.8138
ROC-AUC: 0.8910
</code></pre>

<pre><code class="language-python">from pyspark.ml.feature import NGram, VectorAssembler

def build_ngrams_wocs(inputCol=[&quot;text&quot;,&quot;target&quot;], n=3):
    tokenizer = [Tokenizer(inputCol=&quot;text&quot;, outputCol=&quot;words&quot;)]
    ngrams = [
        NGram(n=i, inputCol=&quot;words&quot;, outputCol=&quot;{0}_grams&quot;.format(i))
        for i in range(1, n + 1)
    ]

    cv = [
        CountVectorizer(vocabSize=5460,inputCol=&quot;{0}_grams&quot;.format(i),
            outputCol=&quot;{0}_tf&quot;.format(i))
        for i in range(1, n + 1)
    ]
    idf = [IDF(inputCol=&quot;{0}_tf&quot;.format(i), outputCol=&quot;{0}_tfidf&quot;.format(i), minDocFreq=5) for i in range(1, n + 1)]

    assembler = [VectorAssembler(
        inputCols=[&quot;{0}_tfidf&quot;.format(i) for i in range(1, n + 1)],
        outputCol=&quot;features&quot;
    )]
    label_stringIdx = [StringIndexer(inputCol = &quot;target&quot;, outputCol = &quot;label&quot;)]
    lr = [LogisticRegression(maxIter=100)]
    return Pipeline(stages=tokenizer + ngrams + cv + idf+ assembler + label_stringIdx+lr)
</code></pre>

<pre><code class="language-python">trigramwocs_pipelineFit = build_ngrams_wocs().fit(train_set)
predictions_wocs = trigramwocs_pipelineFit.transform(val_set)
accuracy_wocs = predictions_wocs.filter(predictions_wocs.label == predictions_wocs.prediction).count() / float(val_set.count())
roc_auc_wocs = evaluator.evaluate(predictions_wocs)

# print accuracy, roc_auc
print(&quot;Accuracy Score: {0:.4f}&quot;.format(accuracy_wocs))
print(&quot;ROC-AUC: {0:.4f}&quot;.format(roc_auc_wocs))
</code></pre>

<pre><code>Accuracy Score: 0.8145
ROC-AUC: 0.8877
</code></pre>

<pre><code class="language-python">predictions_wocs_t3 = trigramwocs_pipelineFit.transform(train_set)
accuracy_wocs_t3 = predictions_wocs_t3.filter(predictions_wocs_t3.label == predictions_wocs_t3.prediction).count() / float(train_set.count())
accuracy_wocs_t3
</code></pre>

<pre><code>0.8172835088436765
</code></pre>

<pre><code class="language-python">test_predictions = trigramwocs_pipelineFit.transform(test_set)
test_accuracy = test_predictions.filter(test_predictions.label == test_predictions.prediction).count() / float(test_set.count())
test_roc_auc = evaluator.evaluate(test_predictions)

# print accuracy, roc_auc
print(&quot;Accuracy Score: {0:.4f}&quot;.format(test_accuracy))
print(&quot;ROC-AUC: {0:.4f}&quot;.format(test_roc_auc))


</code></pre>

<pre><code>Accuracy Score: 0.8099
ROC-AUC: 0.8832
</code></pre>

<pre><code class="language-python">
</code></pre>

		</div>
		
<div class="post__tags tags clearfix">
	<svg class="tags__icon icon icon-tag" width="16" height="16" viewBox="0 0 32 32"><path d="M32 19c0 1-1 2-1 2L21 31s-1 1-2 1-2-1-2-1L2 16c-1-1-1.4-2-1.4-2S0 12.5 0 11V3C0 1.5.8.8.8.8S1.5 0 3 0h8c1.5 0 3 .6 3 .6S15 1 16 2l15 15s1 1 1 2zM7 10a3 3 0 1 0 0-6 3 3 0 0 0 0 6z"/></svg>
	<ul class="tags__list">
		<li class="tags__item"><a class="tags__link btn" href="/tags/spark/" rel="tag">Spark</a></li>
		<li class="tags__item"><a class="tags__link btn" href="/tags/idf/" rel="tag">IDF</a></li>
		<li class="tags__item"><a class="tags__link btn" href="/tags/logistic-regression/" rel="tag">Logistic Regression</a></li>
	</ul>
</div>
	</article>
</main>

<div class="authorbox clearfix">
	<figure class="authorbox__avatar">
		<img alt="Shuai avatar" src="/img/avatar.png" class="avatar" height="90" width="90">
	</figure>
	<div class="authorbox__header">
		<span class="authorbox__name">About Shuai</span>
	</div>
	<div class="authorbox__description">
		Shuai Ma&#39;s true identity is unknown. Maybe he is a successful blogger or writer. Nobody knows it.
	</div>
</div>

<nav class="post-nav flex">
	<div class="post-nav__item post-nav__item--prev">
		<a class="post-nav__link" href="/posts/my-first/" rel="prev"><span class="post-nav__caption">«&thinsp;Previous</span><p class="post-nav__post-title">My First</p></a>
	</div>
	<div class="post-nav__item post-nav__item--next">
		<a class="post-nav__link" href="/posts/englishposts/" rel="next"><span class="post-nav__caption">Next&thinsp;»</span><p class="post-nav__post-title">English Posts List</p></a>
	</div>
</nav>


			</div>
			<aside class="sidebar"><div class="widget-search widget">
	<form class="widget-search__form" role="search" method="get" action="https://google.com/search">
		<label>
			<input class="widget-search__field" type="search" placeholder="SEARCH..." value="" name="q" aria-label="SEARCH...">
		</label>
		<input class="widget-search__submit" type="submit" value="Search">
		<input type="hidden" name="sitesearch" value="/" />
	</form>
</div>
<div class="widget-categories widget">
	<h4 class="widget__title">Categories</h4>
	<div class="widget__content">
		<ul class="widget__list">
			<li class="widget__item"><a class="widget__link" href="/categories/english-posts">English posts</a></li>
			<li class="widget__item"><a class="widget__link" href="/categories/%e4%b8%ad%e6%96%87%e6%96%87%e7%ab%a0">中文文章</a></li>
		</ul>
	</div>
</div>
<div class="widget-taglist widget">
	<h4 class="widget__title">Tags</h4>
	<div class="widget__content">
		<a class="widget-taglist__link widget__link btn" href="/tags/basic-elements" title="Basic elements">Basic elements</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/css" title="Css">Css</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/development" title="Development">Development</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/go" title="Go">Go</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/golang" title="Golang">Golang</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/html" title="Html">Html</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/hugo" title="Hugo">Hugo</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/idf" title="Idf">Idf</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/logistic-regression" title="Logistic regression">Logistic regression</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/spark" title="Spark">Spark</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/templates" title="Templates">Templates</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/themes" title="Themes">Themes</a>
	</div>
</div>
<div class="widget-recent widget">
	<h4 class="widget__title">Recent Posts</h4>
	<div class="widget__content">
		<ul class="widget__list">
			<li class="widget__item"><a class="widget__link" href="/posts/chineseposts/">中文文章&#34;</a></li>
			<li class="widget__item"><a class="widget__link" href="/posts/englishposts/">English Posts List</a></li>
			<li class="widget__item"><a class="widget__link" href="/posts/mymd/">SparkRDD及SparkML在语义分析中的应用</a></li>
			<li class="widget__item"><a class="widget__link" href="/posts/my-first/">My First</a></li>
			<li class="widget__item"><a class="widget__link" href="/posts/basic-elements/">Basic HTML Elements</a></li>
		</ul>
	</div>
</div>
</aside>
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2019 大数据时代 | Era of Big Data.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
	</div>
<script async defer src="/js/menu.js"></script></body>
</html>